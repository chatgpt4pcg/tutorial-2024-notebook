{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594ee5d4b45a9ed4",
   "metadata": {},
   "source": [
    "# Prompt Engineering for Science Birds Level Generation and Beyond Tutorial\n",
    "At [IEEE CoG 2024 Tutorial](https://2024.ieee-cog.org/cog-2024-tutorials/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0ed6703068c50",
   "metadata": {},
   "source": [
    "In this tutorial, our main goal is to implement tree-of-thought prompting for generating levels in the Science Birds game. We will use the [the LLM4PCG Python package](https://github.com/Pittawat2542/llm4pcg-python) to interact with the LLM model. This package is a modified version of the original [ChatGPT4PCG](https://github.com/chatgpt4pcg/chatgpt4pcg-python) package, designed for the [ChatGPT4PCG](https://chatgpt4pcg.github.io) competition.\n",
    " \n",
    "To provide basics of prompt engineering, we will go through some basic prompt engineering, which will serve as a foundation for implementing tree-of-thought prompting.\n",
    "\n",
    "Basic prompt engineering includes:\n",
    "1. Zero-shot prompting\n",
    "2. Zero-shot chain-of-thought prompting\n",
    "3. Few-shot prompting\n",
    "4. Tree-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff68c23a1e608d9",
   "metadata": {},
   "source": [
    "_**NOTE:** Please ensure that you follow an instruction in **`README.md`** to set up the environment before running the code._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa860dbcf674ca",
   "metadata": {},
   "source": [
    "# Mini-LLM4PCG Competition\n",
    "\n",
    "![LLM4PCG logo](images/logo.jpg)\n",
    "\n",
    "The mini LLM4PCG competition is a simplified version of the ChatGPT4PCG competition. The goal is to generate levels in the Science Birds game using the LLM model. The competition consists of three characters: `I`, `L`, and `U`. Participants are required to generate `10` levels for each character using the LLM model. The levels are evaluated based on stability and similarity to the target character. You are required to submit the zipped folder containing all the generated responses (see image below).\n",
    "\n",
    "![Folder Structure](images/folder.png)\n",
    "\n",
    "**If you are interested in participating into this mini LLM4PCG competition. Please make a submission via this form [here](https://forms.gle/3WDW4acnEMidcqbu6) before Tue Aug 6, 2024 11:45AM. We will make an announcement on a final ranking via the [website](https://chatgpt4pcg.github.io/tutorial).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f056d1a45a1ca4b",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "![Conversion Tool](images/converter.png)\n",
    "\n",
    "Since the actual score of the competition needs to be run through an evaluation pipeline and may be costly to execute, we recommend participants to utilize our online converter, which can convert the generated response into a visual representation that can approximate the actual level. The converter can be found [here](https://chatgpt4pcg.github.io/tools/converter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a4e557fa1cfe67",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "The LLM4PCG Python package requires at least Python 3.11. You may encounter issues if you use an older version of Python. However, instructions for setting up a virtual environment are provided in the `README.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Install the LLM4PCG package, which provides the necessary functions for interacting with the LLM model via the competition API.\n",
    "!pip install llm4pcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d193a0bafa7829",
   "metadata": {},
   "source": [
    "## Importing the packages\n",
    "\n",
    "Let's run the cell below to import all the packages that you will need during this tutorial.\n",
    "\n",
    "- [llm4pcg](https://pypi.org/project/llm4pcg/) is a python package containing required and utility functions of ChatGPT4PCG competition, but modified to support local LLMs that compatible with OpenAI API interface.\n",
    "- [re](https://docs.python.org/3/library/re.html) is a module for working with regular expressions in Python.\n",
    "- [pathlib](https://docs.python.org/3/library/pathlib.html) provides object-oriented filesystem paths for different operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a8d99ec8017f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from llm4pcg.competition import chat_with_llm, run_evaluation\n",
    "from llm4pcg.models.trial_context import TrialContext\n",
    "from llm4pcg.models.trial_loop import TrialLoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523ef84b75bd65e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2064b8faa968d72e",
   "metadata": {},
   "source": [
    "### How to check the port of the local server on LM Studio?\n",
    "\n",
    "> In case you don't have LM Studio installed, you can follow the instructions on this [page](https://chatgpt4pcg.github.io/tutorial) to set up the program and the local server.\n",
    "\n",
    "1. From the home screen of LM Studio, navigate to the \"Local Server\" tab.\n",
    "\n",
    "![Local Server Tab](images/port-1.png)\n",
    "\n",
    "2. Check the port number in \"Server Port\" section.\n",
    "\n",
    "![Server Port](images/port-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe946b1693e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTERS = [\"I\", \"L\", \"U\"]  # Change this to the list of targets you want to evaluate. Reduce the number of characters for faster testing. Please note that the `CHARACTERS` should be [\"I\", \"L\", \"U\"] in case you want to submit your results to the mini-competition.\n",
    "NUM_TRIALS = 1  # Adjust the number of trials as needed. You may want to lower it for faster testing. Please note that the `NUM_TRIALS` should be 10 in case you want to submit your results to the mini-competition.\n",
    "MODEL_NAME = \"local-model\"  # This variable may need to be changed to work with some other programs that are not current in LM Studio v0.2.27.\n",
    "PORT = 1234  # TODO: Please refer to the instructions above on how to check the port of the local server of LM Studio and change it if necessary.\n",
    "LOCAL_MODEL_BASE_URL = f\"http://localhost:{PORT}/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cd1e28e35bbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper system prompt for improving structured output generation. You may experiment with modifying this prompt to enhance the output.\n",
    "SYSTEM_PROMPT = \"Output in Markdown code block format (between ``` and ```). The last code block must contain all the \\\n",
    "necessary code required to produce a level. Output only the 'drop_block' function with proper arguments, without any \\\n",
    "other code. You do not need to define the 'drop_block' function or any other functions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf811d1109cdea4d",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting is a technique used with large language models (LLMs) where the model is given a task or query without any specific examples or prior training for that particular task. Here's a concise description:\n",
    "\n",
    "Zero-shot prompting involves providing an LLM with a prompt or instruction for a task it hasn't been explicitly trained on, relying on the model's pre-existing knowledge and capabilities to generate an appropriate response. This technique allows users to leverage the model's broad understanding of language and concepts to perform new tasks without additional training or fine-tuning.\n",
    "\n",
    "Key aspects of zero-shot prompting include:\n",
    "- No examples are provided in the prompt.\n",
    "- It relies on the model's pre-training and ability to understand and follow instructions.\n",
    "- It's particularly useful for quick tasks or exploring an LLM's capabilities without extensive prompt engineering.\n",
    "\n",
    "Zero-shot prompting showcases the versatility of modern LLMs but may not always produce optimal results for all types of tasks or queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928dfec27e99b35",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt\n",
    "\n",
    "This prompt first start with an instruction of the ChatGPT4PCG task and provide a brief description of what is expected. Next part utilizes role prompting (`1. Role`) to condition the model and provides basic ideas of the task. The rest are additional information about the competition and the task. The prompt file is located at `prompts/task-zero-shot.txt`.\n",
    "\n",
    "You are encouraged to modify the prompt to better suit your needs and potentially improve the performance. For example, rephrasing the instructions, removing, or adding more details to the task description.\n",
    "\n",
    "<code>\n",
    "    Use `drop_block()` function to generate a stable structure that looks like the character <span style=\"color:green;background-color:yellow;font-weight:bold;\">{object}</span>—the goal—and meets all the hard constraints. \n",
    "    Dropping position and order are crucial, and they must be determined using techniques in the block-stacking problem.<br>\n",
    "    1. Role\n",
    "    You are a player of the Tetris game who aims to generate a structure that meets the goal while satisfying all the hard constraints.<br>   \n",
    "    2. Definitions\n",
    "    Slots: The map's width is equally partitioned into W slots where W = 20, with slots 0 and 19 being the most left and right, respectively.\n",
    "    Layers: The map's height is equally partitioned into H layers where H = 16, with layers 0 and 15 being the bottom and top layers, respectively.\n",
    "    Base: The bottom of the map, i.e., layer 0.\n",
    "    Map Initialization:\n",
    "    # initialize the structure as an empty WxH grid\n",
    "    structure = [[' ']*W for _ in range(H)]<br>\n",
    "    3. Environment\n",
    "    There are three block types as follows:\n",
    "    b11, a square block whose width is 1 unit and height is 1 unit\n",
    "    b31, a horizontal block whose width is 3 units and height is 1 unit\n",
    "    b13, a vertical block whose width is 1 unit and height is 3 units<br>\n",
    "    4. Tool\n",
    "    Use the following function to vertically drop a block from layer H such that its center is at slot x_position and drop earlier blocks representing more bottom parts of the structure.\n",
    "    drop_block(block_type: str, x_position: int),\n",
    "    where block_type is a block type, and x_position is the slot number from 0 to W-1 where the block center is aligned.  After vertically falling down, the block will end up at either the layer on top of the base or a previously dropped block. This function is defined as follows:\n",
    "    def drop_block(block_type: str, x_position: int):\n",
    "        # block_type is the block type, x_position is the slot number from 0 to W-1 where the block center is aligned<br>   \n",
    "        # initialize the drop position at the top of the map\n",
    "        drop_pos = (H-1, x_position)<br>\n",
    "        # drop the block from the top and move it down until it lands on the base or another block\n",
    "        while drop_pos[0] > 0:\n",
    "            drop_pos = (drop_pos[0]-1, x_position)\n",
    "            if structure[drop_pos[0]+1][drop_pos[1]] != ' ':\n",
    "                break<br>\n",
    "        # place the block on the structure\n",
    "        structure[drop_pos[0]][drop_pos[1]] = block_type<br>\n",
    "    5. Constraints:\n",
    "    The relevant constraints are given in the following.<br>\n",
    "    5.1 No boundary intrusion: This is a soft constraint that should be met if possible. Namely, blocks should not intrude on the boundary of the map. In other words, the area of intrusion regions should be zero.\n",
    "    ---\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695de360303f8646",
   "metadata": {},
   "source": [
    "For the ChatGPT4PCG competition, a class needs to be implemented that inherits from the `TrialLoop` class. The `run` function should be implemented to execute the evaluation. This `run` function is expected to return the generated text in Markdown format, which includes a code block demonstrating the usage of the `drop_block` function and its arguments generated by ChatGPT.\n",
    "\n",
    "For additional information on the `llm4pcg` package and its available functions and classes, please visit the [documentation](https://github.com/Pittawat2542/llm4pcg-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70db0c474748f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotPrompting(TrialLoop):\n",
    "    # ^ If you modify this class name, please also modify the corresponding function name in the run_evaluation function.\n",
    "    @staticmethod\n",
    "    def run(ctx: TrialContext, target_character: str) -> str:\n",
    "        \"\"\"\n",
    "        Runs the zero-shot prompting.\n",
    "        :param ctx: The trial context.\n",
    "        :param target_character: The target character.\n",
    "        :return: The generated text.\n",
    "        \"\"\"\n",
    "        # Read the prompt text file\n",
    "        prompt_template = open(Path(\"prompts/task-zero-shot.txt\"), \"r\").read()\n",
    "\n",
    "        # Generate the response from the LLM. We provide a system and a user prompt.\n",
    "        responses = chat_with_llm(ctx, [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            # This system prompt is optional but can help improve the output.\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(\n",
    "                object=target_character  # Replace the `{object}` placeholder in the prompt with the target character.\n",
    "            )}])\n",
    "\n",
    "        response = responses[\n",
    "            0]  # Since the `chat_with_llm` function returns a list of responses, we select the first one as we only generate one response.\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba35064c6b1fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to run the evaluation for the ChatGPT4PCG task. The first parameter is the folder name, the second is the class that is to implement the prompt, and the rest are the parameters for the prompt engineering, zero-shot prompting in this case.\n",
    "run_evaluation(\"zero_shot\", ZeroShotPrompting, characters=CHARACTERS, num_trials=NUM_TRIALS,\n",
    "               model_name=MODEL_NAME, local_model_base_url=LOCAL_MODEL_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99b566394799b4",
   "metadata": {},
   "source": [
    "## Zero-Shot Chain-of-Thought Prompting\n",
    "\n",
    "Zero-shot chain-of-thought (zero-shot CoT) prompting is a technique used with LLMs that encourages step-by-step reasoning without providing specific examples.\n",
    "\n",
    "The key aspects of zero-shot CoT prompting are:\n",
    "- It involves appending the phrase \"Let's think step by step\" (or similar variations) to the end of a query or prompt.\n",
    "- This simple addition guides the LLM to generate a chain of thought, breaking down its reasoning process into intermediate steps.\n",
    "- It aims to improve the model's performance on complex reasoning tasks, including arithmetic, commonsense, and symbolic reasoning.\n",
    "- Unlike few-shot CoT prompting, zero-shot CoT prompting doesn't require manually crafted examples, making it more versatile and easier to implement.\n",
    "- The technique has shown to be effective with larger language models (>100 billion parameters).\n",
    "- While generally effective, zero-shot CoT prompting may not always be as accurate as few-shot CoT prompting, especially for more complex tasks.\n",
    "\n",
    "Zero-shot CoT prompting represents a simple yet powerful approach to enhance LLMs' reasoning capabilities without the need for task-specific training or examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad73b6f5e0e576d",
   "metadata": {},
   "source": [
    "## Zero-Shot CoT Prompt\n",
    "\n",
    "This prompt are largely the same as the zero-shot prompt, but with the addition of the phrase \"Let's think step by step\" at the end of the prompt. This phrase is crucial for guiding the model to generate a chain of thought and potentially improve the performance. Please note that we place the phrase at the end of the prompt following the original paper. The prompt file is located at `prompts/task-zero-shot-cot.txt`.\n",
    "\n",
    "As usual, you are encouraged to modify the prompt to better suit your needs and potentially improve the performance. For example, you may opt to go with few-shot chain-of-thought prompting instead by providing explicit reasoning steps and examples in the prompt instead of relying on the model's reasoning.\n",
    "\n",
    "```\n",
    "<zero-shot-prompt>\n",
    "Let's think step-by-step.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508a28d944e3154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotCoTPrompting(TrialLoop):\n",
    "    @staticmethod\n",
    "    def run(ctx: TrialContext, target_character: str) -> str:\n",
    "        \"\"\"\n",
    "        Runs the zero-shot chain-of-thought prompting.\n",
    "        :param ctx: The trial context.\n",
    "        :param target_character: The target character.\n",
    "        :return: The generated text.\n",
    "        \"\"\"\n",
    "        # Similar to the previous zero-shot prompting, but this time we are reading a different prompt file.\n",
    "        prompt_template = open(Path(\"prompts/task-zero-shot-cot.txt\"), \"r\").read()\n",
    "\n",
    "        responses = chat_with_llm(ctx, [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(\n",
    "                object=target_character\n",
    "            )}])\n",
    "\n",
    "        response = responses[0]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b8b0af58f7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\"zero_shot_cot\", ZeroShotCoTPrompting, characters=CHARACTERS, num_trials=NUM_TRIALS,\n",
    "               model_name=MODEL_NAME, local_model_base_url=LOCAL_MODEL_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9435ac16743c99",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting is a technique used with LLMs where the model is given a small number of examples or demonstrations within the prompt to guide its response for a specific task.\n",
    "\n",
    "Key aspects of few-shot prompting include:\n",
    "- Providing 2-5 input-output examples in the prompt to illustrate the desired task or format.\n",
    "- Leveraging the LLM's ability to learn from these examples and generalize to new inputs (in-context learning).\n",
    "- Enabling the model to quickly adapt to new tasks without fine-tuning or retraining.\n",
    "- Particularly effective for tasks like classification, translation, or text generation in specific styles.\n",
    "- Balancing between too few examples (insufficient guidance) and too many (overwhelming the model or context limit).\n",
    "- Often more effective than zero-shot prompting, especially for complex or nuanced tasks.\n",
    "\n",
    "Few-shot prompting takes advantage of LLMs' in-context learning capabilities, allowing them to perform new tasks with minimal task-specific training data. This technique has become an important tool in prompt engineering for enhancing LLM performance across various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b30e2d57d9506",
   "metadata": {},
   "source": [
    "### Few-Shot Prompt\n",
    "\n",
    "This prompt shares similarities with the zero-shot prompt but includes an additional section (`6. Examples`) containing three examples (3-shot prompting). These examples are provided to demonstrate the task and guide the model's response, i.e., allowing the model to perform in-context learning. These examples are also formatted to reflect what the model should generate.\n",
    " The prompt file is located at `prompts/task-few-shot.txt`.\n",
    "\n",
    "As usual, you are encouraged to modify the prompt to better suit your needs and potentially improve the performance. For example, adding more examples or changing the style of the examples.\n",
    "\n",
    "<code>\n",
    "    <span style=\"color:blue;font-weight:bold;\">&ltzero-shot prompt&gt</span>\n",
    "    6. Examples\n",
    "    6.1. Input: \"G\"\n",
    "    Output:\n",
    "    ```\n",
    "    drop_block(\"b31\",8)\n",
    "    drop_block(\"b31\",11)\n",
    "    drop_block(\"b11\",6)\n",
    "    drop_block(\"b31\",7)\n",
    "    drop_block(\"b31\",12)\n",
    "    drop_block(\"b31\",11)\n",
    "    drop_block(\"b11\",13)\n",
    "    drop_block(\"b13\",14)\n",
    "    drop_block(\"b13\",8)\n",
    "    drop_block(\"b31\",6)\n",
    "    drop_block(\"b31\",6)\n",
    "    drop_block(\"b31\",6)\n",
    "    drop_block(\"b31\",7)\n",
    "    drop_block(\"b11\",6)\n",
    "    drop_block(\"b31\",8)\n",
    "    drop_block(\"b11\",7)\n",
    "    drop_block(\"b31\",9)\n",
    "    ```\n",
    "    6.2. Input: \"Q\"\n",
    "    Output:\n",
    "    ```\n",
    "    drop_block(\"b31\",8)\n",
    "    drop_block(\"b11\",6)\n",
    "    drop_block(\"b31\",11)\n",
    "    drop_block(\"b31\",14)\n",
    "    drop_block(\"b11\",10)\n",
    "    drop_block(\"b31\",12)\n",
    "    drop_block(\"b31\",6)\n",
    "    drop_block(\"b13\",7)\n",
    "    drop_block(\"b11\",7)\n",
    "    drop_block(\"b11\",7)\n",
    "    drop_block(\"b13\",11)\n",
    "    drop_block(\"b11\",11)\n",
    "    drop_block(\"b11\",11)\n",
    "    drop_block(\"b31\",13)\n",
    "    drop_block(\"b31\",5)\n",
    "    drop_block(\"b31\",13)\n",
    "    drop_block(\"b31\",5)\n",
    "    drop_block(\"b31\",13)\n",
    "    drop_block(\"b31\",5)\n",
    "    drop_block(\"b31\",13)\n",
    "    drop_block(\"b31\",5)\n",
    "    drop_block(\"b31\",13)\n",
    "    drop_block(\"b31\",5)\n",
    "    drop_block(\"b31\",12)\n",
    "    drop_block(\"b31\",6)\n",
    "    drop_block(\"b11\",4)\n",
    "    drop_block(\"b31\",11)\n",
    "    drop_block(\"b11\",5)\n",
    "    drop_block(\"b31\",7)\n",
    "    drop_block(\"b11\",6)\n",
    "    drop_block(\"b31\",8)\n",
    "    drop_block(\"b11\",10)\n",
    "    drop_block(\"b11\",11)\n",
    "    ```\n",
    "    6.3. Input: \"S\"\n",
    "    Output:\n",
    "    ```\n",
    "    drop_block('b31', 10)\n",
    "    drop_block('b31', 11)\n",
    "    drop_block('b31', 10)\n",
    "    drop_block('b31', 9)\n",
    "    drop_block('b31', 10)\n",
    "    ```\n",
    "    ---\n",
    "    Input: \"<span style=\"color:green;background-color:yellow;font-weight:bold;\">{object}</span>\"\n",
    "    Output:\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a78c69320bb52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotPrompting(TrialLoop):\n",
    "    @staticmethod\n",
    "    def run(ctx: TrialContext, target_character: str) -> str:\n",
    "        \"\"\"\n",
    "        Runs the few-shot prompting.\n",
    "        :param ctx: The trial context.\n",
    "        :param target_character: The target character.\n",
    "        :return: The generated text.\n",
    "        \"\"\"\n",
    "        # Similar to the previous zero-shot prompting, but this time we are reading a different prompt file.\n",
    "        prompt_template = open(Path(\"prompts/task-few-shot.txt\"), \"r\").read()\n",
    "\n",
    "        responses = chat_with_llm(ctx, [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(\n",
    "                object=target_character\n",
    "            )}])\n",
    "\n",
    "        response = responses[0]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a3ac69c3e2cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\"few_shot\", FewShotPrompting, characters=CHARACTERS, num_trials=NUM_TRIALS,\n",
    "               model_name=MODEL_NAME, local_model_base_url=LOCAL_MODEL_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe217100dcbbee",
   "metadata": {},
   "source": [
    "## Tree-of-Thought Prompting\n",
    "\n",
    "Tree-of-Thought (ToT) prompting is an advanced technique for enhancing the problem-solving capabilities of LLMs. Key aspects of ToT prompting include:\n",
    "- Structured reasoning: ToT represents the problem-solving process as a tree of \"thoughts,\" where each thought is an intermediate step towards the solution.\n",
    "- Multiple paths: Unlike linear approaches, ToT generates and explores multiple reasoning paths simultaneously, allowing for a more comprehensive problem-solving approach.\n",
    "- Self-evaluation: The LLM evaluates the promise of different thought branches, guiding the search through the reasoning tree.\n",
    "- Deliberate search: ToT incorporates search algorithms like breadth-first or depth-first search to systematically explore the tree of thoughts.\n",
    "- Improved performance: ToT has shown significant improvements over other prompting methods, especially in tasks requiring complex reasoning, planning, or exploration.\n",
    "- No additional training: ToT can be implemented without retraining the LLM, making it a versatile prompt engineering technique.\n",
    "\n",
    "By enabling LLMs to consider multiple possibilities, backtrack when necessary, and make global decisions, ToT prompting mimics more human-like problem-solving strategies, leading to enhanced performance on complex tasks.\n",
    "\n",
    "![Tree-of-Thought Prompting](images/tot.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47279d22-019b-433e-b590-5007147ecfff",
   "metadata": {},
   "source": [
    "### ToT Prompts\n",
    "\n",
    "There are **three** main prompts required for the ToT task: a **`task prompt`** for generating thoughts, an **`evaluation prompt`** for evaluating thoughts and giving numerical scores, and a **`answering prompt`** for combining and formatting accumulated content from the best-performing thoughts at each level.\n",
    "\n",
    "#### Task Prompt\n",
    "The task prompt includes elements from few-shot chain-of-thought prompting, in addition to general task instructions following zero-shot prompting. The additional part involves a reasoning step and an associated example, following the style of few-shot chain-of-thought prompting. The prompt file is located at `tot-prompts/task-tot.txt`.\n",
    "\n",
    "<code>\n",
    "    Use `drop_block()` function to generate a stable structure that looks like the character <span style=\"color:green;background-color:yellow;font-weight:bold;\">{object}</span>—the goal—and meets all the hard constraints. \n",
    "    Dropping position and order are crucial, and they must be determined using techniques in the block-stacking problem.\n",
    "    <span style=\"color:blue;font-weight:bold;\">&ltpart of zero-shot prompt&gt</span>\n",
    "    ---\n",
    "    <span style=\"color:red;\">Let's follow the following steps</span>\n",
    "    <span style=\"color:red;\">1.</span> Generate the base layer of the structure\n",
    "    <span style=\"color:red;\">2.</span> Generate the top layer of the structure\n",
    "    Only perform one step at a time.<br>\n",
    "    Example\n",
    "    Character A:\n",
    "    ```\n",
    "    # Base layer\n",
    "    drop_block('b11', 0)\n",
    "    drop_block('b11', 0)\n",
    "    drop_block('b11', 2)\n",
    "    drop_block('b11', 2)\n",
    "    drop_block('b31', 1)\n",
    "    ```    \n",
    "    ```\n",
    "    # Top layer\n",
    "    drop_block('b11', 0)\n",
    "    drop_block('b11', 2)\n",
    "    drop_block('b31', 1)\n",
    "    ``` \n",
    "    Currently, we have\n",
    "    <span style=\"color:green;background-color:yellow;font-weight:bold;\">{generated_content_so_far}</span>\n",
    "    Next, we will perform the\n",
    "</code>\n",
    "\n",
    "#### Evaluation Prompt\n",
    "The evaluation prompt includes instructions for evaluating the generated thoughts and providing numerical scores. In this case, we evaluate the thoughts based on stability and similarity and instruct the LLM to provide scores between 0 and 10 for each aspect. The prompt file is located at `tot-prompts/evaluation-tot.txt`.\n",
    "\n",
    "<code>\n",
    "    The following code is used to generate a Science Birds level that resembling the uppercase English character: <span style=\"color:green;background-color:yellow;font-weight:bold;\">{object}</span>. \n",
    "    The description of the function utilized for this purpose is given below.<br>\n",
    "    Provide integer scores for the following levels between 0 and 10 for two aspects stability and similarity. Provide the response in the following format.\n",
    "    Stability: &ltscore&gt\n",
    "    Similarity: &ltscore&gt<br>\n",
    "    <span style=\"color:blue;font-weight:bold;\">&ltpart of zero-shot prompt&gt</span><br>\n",
    "    3. Generated content to be evaluated\n",
    "    <span style=\"color:green;background-color:yellow;font-weight:bold;\">{generated_content_so_far}</span><br>\n",
    "    Scores:\n",
    "</code>\n",
    "\n",
    "#### Answering Prompt\n",
    "The answering prompt includes instructions for formatting the final response based on the evaluated scores. The prompt file is located at `tot-prompts/answer-tot.txt`.\n",
    "\n",
    "<code>\n",
    "    <span style=\"color:green;background-color:yellow;font-weight:bold;\">{generated_content}</span><br>\n",
    "    Give a final code in Markdown format that generates the target structure.<br>\n",
    "    Output:\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b60182359a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeOfThoughtPrompting(TrialLoop):\n",
    "    @staticmethod\n",
    "    def extract_scores(scores_str: str):\n",
    "        scores_str = scores_str.lower()\n",
    "        stability_pattern = r\".*stability: (10|\\d).*\"\n",
    "        similarity_pattern = r\".*similarity: (10|\\d).*\"\n",
    "        stability = 0\n",
    "        similarity = 0\n",
    "        if stability_match := re.search(stability_pattern, scores_str):\n",
    "            stability = int(stability_match.group(1))\n",
    "        if similarity_match := re.search(similarity_pattern, scores_str):\n",
    "            similarity = int(similarity_match.group(1))\n",
    "        return stability, similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def tot(ctx: TrialContext, target_character: str) -> str:\n",
    "        # Maximum depth of the tree are associate with steps in the provided prompt. It can be adjusted as needed, but also potentially provide better results.\n",
    "        # Please remember that you also need to adjust the prompt to provide the number of step equal to number of maximum depth. Higher numbers of levels may require more computation time\n",
    "        max_depth = 2 \n",
    "        branching_factor = 2  # Number of thoughts to generate at each level. It can be adjusted as needed. Higher numbers may require more computation time, but also potentially provide better results.\n",
    "\n",
    "        # A variable to hold accumulated content of the best thought at each level\n",
    "        current_content = \"\"\n",
    "\n",
    "        # Read the prompt text files\n",
    "        task_prompt_template = open(Path(\"tot-prompts/task-tot.txt\"), \"r\").read()\n",
    "        evaluation_prompt_template = open(Path(\"tot-prompts/evaluate-tot.txt\"), \"r\").read()\n",
    "        answer_prompt_template = open(Path(\"tot-prompts/answer-tot.txt\"), \"r\").read()\n",
    "\n",
    "        try:\n",
    "            # TODO: Implement this function (hints below and complete code is in `tot-final.py`)\n",
    "            # Loop until reaching the maximum depth\n",
    "            # | 1. Perform the task to generate {branching_factor} thoughts\n",
    "            # | 2. Evaluate each thought and select the best one\n",
    "            # | 3. Format the final response in a correct format and return it\n",
    "\n",
    "            ### START CODE HERE ### \n",
    "            \n",
    "\n",
    "            ### END CODE HERE ### \n",
    "        except (ValueError, TimeoutError) as e:\n",
    "            print(e)\n",
    "            return current_content\n",
    "\n",
    "    @staticmethod\n",
    "    def run(ctx: TrialContext, target_character: str) -> str:\n",
    "        \"\"\"\n",
    "        Runs the tree-of-thought prompting.\n",
    "        :param ctx: The trial context.\n",
    "        :param target_character: The target character.\n",
    "        :return: The generated text.\n",
    "        \"\"\"\n",
    "        final_response = TreeOfThoughtPrompting.tot(ctx, target_character)\n",
    "\n",
    "        return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382135c-7d17-4b34-bdf2-9cb75a462c85",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "\n",
    "    \n",
    "```python\n",
    "# Loop until reaching the maximum depth\n",
    "for i in range(max_depth):\n",
    "    # | 1. Perform the task to generate {branching_factor} thoughts\n",
    "    responses = []\n",
    "\n",
    "    for j in range(branching_factor):\n",
    "        res = \n",
    "        responses.append(res[0])\n",
    "\n",
    "    # | 2.1. Evaluate each thought ...\n",
    "    scores = []\n",
    "    for response in responses:\n",
    "        score = \n",
    "\n",
    "        evaluation_result =\n",
    "        scores.append(evaluation_result)\n",
    "\n",
    "    # | 2.2. ... and select the best one\n",
    "    best_performing_thought = \n",
    "    current_content += \n",
    "\n",
    "# | 3. Format the final response in a correct format and return it\n",
    "final_response = \n",
    "\n",
    "return final_response[0]\n",
    "```\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for code</b></font></summary>\n",
    "   \n",
    "```python\n",
    "# Loop until reaching the maximum depth\n",
    "for i in range(max_depth):\n",
    "    # | 1. Perform the task to generate {branching_factor} thoughts\n",
    "    responses = []\n",
    "\n",
    "    for j in range(branching_factor):\n",
    "        res = chat_with_llm(ctx, [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": task_prompt_template.format(\n",
    "                object=target_character,\n",
    "                generated_content_so_far=current_content == \"\" and \"nothing\" or current_content,\n",
    "            )}])\n",
    "        responses.append(res[0])\n",
    "\n",
    "    # | 2.1. Evaluate each thought ...\n",
    "    scores = []\n",
    "    for response in responses:\n",
    "        score = chat_with_llm(ctx, [\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt_template.format(\n",
    "                object=target_character,\n",
    "                generated_content_so_far=f\"{current_content}\\n{response}\"\n",
    "            )}])[0]\n",
    "\n",
    "        evaluation_result = (response, TreeOfThoughtPrompting.extract_scores(score))\n",
    "        scores.append(evaluation_result)\n",
    "\n",
    "    # | 2.2. ... and select the best one\n",
    "    best_performing_thought = sorted(scores, key=lambda x: sum(x[1]), reverse=True)\n",
    "    current_content += f\"{best_performing_thought[0][0]}\\n\"\n",
    "\n",
    "# | 3. Format the final response in a correct format and return it\n",
    "final_response = chat_with_llm(ctx, [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": answer_prompt_template.format(\n",
    "        generated_content=current_content\n",
    "    )}\n",
    "])\n",
    "\n",
    "return final_response[0]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5260bf3a2f3cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit exceeded. 134.79854710499967 > 120\n"
     ]
    }
   ],
   "source": [
    "run_evaluation(\"tot\", TreeOfThoughtPrompting, characters=CHARACTERS, num_trials=NUM_TRIALS,\n",
    "               model_name=MODEL_NAME, local_model_base_url=LOCAL_MODEL_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e4769-4e25-40c5-a1e9-7a13659928b1",
   "metadata": {},
   "source": [
    "<code>\n",
    "    <span style=\"color:red\"><zero-shot-prompt></span>  \n",
    "</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
